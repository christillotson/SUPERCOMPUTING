My motivation for learning to work with high-performance computing stems mainly from a desire to expand my skillset. I am graduating soon and entering the job market, and I want to make sure I am well prepared for whatever role I end up in. Most likely, I will be doing some form of data science, and it is not unlikely that I could be tasked with running my computing on remote supercomputers. In the past, I have been exposed to this concept while working during internships, but it seemed mysterious to me and so I avoided it. I want to get more comfortable to the extent that this is not just another skill on my resume but a skill that I can actually use regularly in a job.

My goals for this course are to develop the muscle-memory necessary to not feel like I have jumped in the deep end of a pool if I am asked to use remote computing clusters. Additionally, I would like to flesh out my personal Github / come out of this with a nice final project to show off.

My plan of attack for this semester includes firstly discussing the material with my friends in the class to make sure we all understand the concepts and expectations. Secondly, I will make sure to do the readings even when it doesn't seem I will be tested on the content or that the readings are optional, because I know it is important to expand my learning. I also intend to take detailed notes (I prefer paper notes) to refer to during quizzes and review regularly and while doing homework, because this is how I remember information best. 

My folder structure for this assignment is good for reproducible research for several reasons. Firstly, the organization of a docs folder and a src folder is important for anyone looking at a project at a high level - if they do not understand / want to get into the particulars of the code, the docs and README should be accessible to have explanatory information on the project. The main.py should be a robust demonstration of the intent of the project and should be easily accessible once in the project's directory, pulling any necessary code from src, which can be investigated by people who are more interested in the inner workings of the project instead of the result. The organization of code within a src directory, especially splitting different groups of functions within it, would allow for easy access of a specific function / group of functions. This means that the project's code is highly modular and easily re-purposed, and will also help with documentation, with the documentation only needing to be established on short snippets of code. I used that structure for the "main" code instead of a simple scripts folder because I felt that a scripts folder would be too cluttered. I included most of the other directories given in the example of a well organized project because they do all play important roles. Results is a good spot for any output statistics, which are often what someone looks for after running an experiment or code. Config would be useful to easily change all variables, as opposed to changing them in multiple places (python files) which all rely on the same variables (such as path names). Logs would also be useful for keeping track of when the code is run, how long it takes, etc, to compare when results aren't what they "should" be, when new code has introduced inefficient computation and runtime, etc. Additionally, I added a figures folder because I felt that the output of figures from a project is always useful for explaining it, and they should have a dedicated place rather than spread all over, for easy access.

My understanding of why we use documented code is primarily so that our research is reproducible. The quote by Karl Popper in the reading, from his 1959 *The Logic of Scientific Discovery*, most succicntly explains why this is important: "non-reproducible single occurrences are of no significance to science". We do research to expand the domain of human knowledge, but our results only expand this knowledge if they seem to be true to the real world. To prove that they are, they must be reproducible. Otherwise, there is no way to verify that this "knowledge" is not just made up. So, when doing computational research tasks, this concept still applies. To fully replicate anything we do computationally, we must document (and provide) the code used for it, as well as all the underlying metadata such as library or software versions, so that the study *can* be replicated.